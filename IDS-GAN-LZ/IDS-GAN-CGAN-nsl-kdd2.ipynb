{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./nsl-kdd\\Attack_Types.csv\n",
      "./nsl-kdd\\d_loss.png\n",
      "./nsl-kdd\\g_loss.png\n",
      "./nsl-kdd\\KDDTest.csv\n",
      "./nsl-kdd\\KDDTrain.csv\n",
      "./nsl-kdd\\nslkdd_profile.html\n",
      "./nsl-kdd\\test.parquet\n",
      "./nsl-kdd\\train.parquet\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('./nsl-kdd'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.autograd import grad\n",
    "from tqdm import tqdm  # tqdm 用于显示进度条\n",
    "\n",
    "\n",
    "import pickle, os\n",
    "from collections import defaultdict\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "# %matplotlib inline\n",
    "\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "# 检查是否有可用的 CUDA 设备\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "\n",
    "data_file_path = './nsl-kdd/'\n",
    "df = pd.read_csv(data_file_path+'KDDTrain.csv')\n",
    "# test = pd.read_csv(data_file_path+'KDDTest.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duration</th>\n",
       "      <th>protocol_type</th>\n",
       "      <th>service</th>\n",
       "      <th>flag</th>\n",
       "      <th>src_bytes</th>\n",
       "      <th>dst_bytes</th>\n",
       "      <th>land</th>\n",
       "      <th>wrong_fragment</th>\n",
       "      <th>urgent</th>\n",
       "      <th>hot</th>\n",
       "      <th>...</th>\n",
       "      <th>dst_host_srv_count</th>\n",
       "      <th>dst_host_same_srv_rate</th>\n",
       "      <th>dst_host_diff_srv_rate</th>\n",
       "      <th>dst_host_same_src_port_rate</th>\n",
       "      <th>dst_host_srv_diff_host_rate</th>\n",
       "      <th>dst_host_serror_rate</th>\n",
       "      <th>dst_host_srv_serror_rate</th>\n",
       "      <th>dst_host_rerror_rate</th>\n",
       "      <th>dst_host_srv_rerror_rate</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>ftp_data</td>\n",
       "      <td>SF</td>\n",
       "      <td>491</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>25</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>udp</td>\n",
       "      <td>other</td>\n",
       "      <td>SF</td>\n",
       "      <td>146</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>private</td>\n",
       "      <td>S0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>26</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>dos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>http</td>\n",
       "      <td>SF</td>\n",
       "      <td>232</td>\n",
       "      <td>8153</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>255</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>http</td>\n",
       "      <td>SF</td>\n",
       "      <td>199</td>\n",
       "      <td>420</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>255</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   duration protocol_type   service flag  src_bytes  dst_bytes  land  \\\n",
       "0         0           tcp  ftp_data   SF        491          0     0   \n",
       "1         0           udp     other   SF        146          0     0   \n",
       "2         0           tcp   private   S0          0          0     0   \n",
       "3         0           tcp      http   SF        232       8153     0   \n",
       "4         0           tcp      http   SF        199        420     0   \n",
       "\n",
       "   wrong_fragment  urgent  hot  ...  dst_host_srv_count  \\\n",
       "0               0       0    0  ...                  25   \n",
       "1               0       0    0  ...                   1   \n",
       "2               0       0    0  ...                  26   \n",
       "3               0       0    0  ...                 255   \n",
       "4               0       0    0  ...                 255   \n",
       "\n",
       "   dst_host_same_srv_rate  dst_host_diff_srv_rate  \\\n",
       "0                    0.17                    0.03   \n",
       "1                    0.00                    0.60   \n",
       "2                    0.10                    0.05   \n",
       "3                    1.00                    0.00   \n",
       "4                    1.00                    0.00   \n",
       "\n",
       "   dst_host_same_src_port_rate  dst_host_srv_diff_host_rate  \\\n",
       "0                         0.17                         0.00   \n",
       "1                         0.88                         0.00   \n",
       "2                         0.00                         0.00   \n",
       "3                         0.03                         0.04   \n",
       "4                         0.00                         0.00   \n",
       "\n",
       "   dst_host_serror_rate  dst_host_srv_serror_rate  dst_host_rerror_rate  \\\n",
       "0                  0.00                      0.00                  0.05   \n",
       "1                  0.00                      0.00                  0.00   \n",
       "2                  1.00                      1.00                  0.00   \n",
       "3                  0.03                      0.01                  0.00   \n",
       "4                  0.00                      0.00                  0.00   \n",
       "\n",
       "   dst_host_srv_rerror_rate   label  \n",
       "0                      0.00  normal  \n",
       "1                      0.00  normal  \n",
       "2                      0.00     dos  \n",
       "3                      0.01  normal  \n",
       "4                      0.00  normal  \n",
       "\n",
       "[5 rows x 42 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing_values = df.isnull().sum()\n",
    "# print(f\"Total missing values: {missing_values}\")\n",
    "\n",
    "# columns_with_missing_values = []\n",
    "\n",
    "# for column, count in missing_values.items():\n",
    "#     if count > 0:\n",
    "#         columns_with_missing_values.append(column)\n",
    "\n",
    "# print(f\"Columns with missing values: {columns_with_missing_values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df  = df.dropna()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# label encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dos': 0, 'normal': 1, 'probe': 2, 'r2l': 3, 'u2r': 4}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duration</th>\n",
       "      <th>protocol_type</th>\n",
       "      <th>service</th>\n",
       "      <th>flag</th>\n",
       "      <th>src_bytes</th>\n",
       "      <th>dst_bytes</th>\n",
       "      <th>land</th>\n",
       "      <th>wrong_fragment</th>\n",
       "      <th>urgent</th>\n",
       "      <th>hot</th>\n",
       "      <th>...</th>\n",
       "      <th>dst_host_srv_count</th>\n",
       "      <th>dst_host_same_srv_rate</th>\n",
       "      <th>dst_host_diff_srv_rate</th>\n",
       "      <th>dst_host_same_src_port_rate</th>\n",
       "      <th>dst_host_srv_diff_host_rate</th>\n",
       "      <th>dst_host_serror_rate</th>\n",
       "      <th>dst_host_srv_serror_rate</th>\n",
       "      <th>dst_host_rerror_rate</th>\n",
       "      <th>dst_host_srv_rerror_rate</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>491</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>25</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>146</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>26</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>232</td>\n",
       "      <td>8153</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>255</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>199</td>\n",
       "      <td>420</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>255</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   duration  protocol_type  service  flag  src_bytes  dst_bytes  land  \\\n",
       "0         0            1.0     20.0   9.0        491          0     0   \n",
       "1         0            2.0     44.0   9.0        146          0     0   \n",
       "2         0            1.0     49.0   5.0          0          0     0   \n",
       "3         0            1.0     24.0   9.0        232       8153     0   \n",
       "4         0            1.0     24.0   9.0        199        420     0   \n",
       "\n",
       "   wrong_fragment  urgent  hot  ...  dst_host_srv_count  \\\n",
       "0               0       0    0  ...                  25   \n",
       "1               0       0    0  ...                   1   \n",
       "2               0       0    0  ...                  26   \n",
       "3               0       0    0  ...                 255   \n",
       "4               0       0    0  ...                 255   \n",
       "\n",
       "   dst_host_same_srv_rate  dst_host_diff_srv_rate  \\\n",
       "0                    0.17                    0.03   \n",
       "1                    0.00                    0.60   \n",
       "2                    0.10                    0.05   \n",
       "3                    1.00                    0.00   \n",
       "4                    1.00                    0.00   \n",
       "\n",
       "   dst_host_same_src_port_rate  dst_host_srv_diff_host_rate  \\\n",
       "0                         0.17                         0.00   \n",
       "1                         0.88                         0.00   \n",
       "2                         0.00                         0.00   \n",
       "3                         0.03                         0.04   \n",
       "4                         0.00                         0.00   \n",
       "\n",
       "   dst_host_serror_rate  dst_host_srv_serror_rate  dst_host_rerror_rate  \\\n",
       "0                  0.00                      0.00                  0.05   \n",
       "1                  0.00                      0.00                  0.00   \n",
       "2                  1.00                      1.00                  0.00   \n",
       "3                  0.03                      0.01                  0.00   \n",
       "4                  0.00                      0.00                  0.00   \n",
       "\n",
       "   dst_host_srv_rerror_rate  label  \n",
       "0                      0.00      1  \n",
       "1                      0.00      1  \n",
       "2                      0.00      0  \n",
       "3                      0.01      1  \n",
       "4                      0.00      1  \n",
       "\n",
       "[5 rows x 42 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OrdinalEncoder, RobustScaler\n",
    "# 处理数据中的字符值\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# \n",
    "df['label'] = label_encoder.fit_transform(df['label'])\n",
    "\n",
    "\n",
    "label_mapping = {l: i for i, l in enumerate(label_encoder.classes_)}\n",
    "print(label_mapping)\n",
    "\n",
    "col = [\"protocol_type\",\"service\",\"flag\"]\n",
    "\n",
    "df[col] = OrdinalEncoder().fit_transform(df[col])\n",
    "\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "to_drop = ['dst_host_srv_serror_rate','num_root','rerror_rate',\n",
    "            'serror_rate','srv_rerror_rate','srv_serror_rate']\n",
    "df.drop(to_drop,axis=1,inplace=True)\n",
    "\n",
    "\n",
    "data_cols = list(df.columns[ df.columns != 'label' ])\n",
    "\n",
    "scaler = RobustScaler(quantile_range=(0.1, 99.9))\n",
    "df[data_cols] = scaler.fit_transform(df[data_cols])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       ...,\n",
       "       [0],\n",
       "       [0],\n",
       "       [0]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# 将数据集分为训练集和测试集\n",
    "x_train, x_test = train_test_split(df, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "train, test = x_train, x_test\n",
    "\n",
    "x_train.to_parquet('./nsl-kdd/train.parquet', engine='pyarrow')\n",
    "x_test.to_parquet('./nsl-kdd/test.parquet', engine='pyarrow')\n",
    "\n",
    "\n",
    "data_cols = list(x_train.columns[ x_train.columns != 'label' ])\n",
    "\n",
    "y_train = x_train.label.values\n",
    "y_test = x_test.label.values\n",
    "\n",
    "\n",
    "att_ind = np.where(x_train.label != label_mapping[\"normal\"])[0]\n",
    "x = x_train[data_cols].values[att_ind]\n",
    "y = y_train[att_ind].reshape(-1,1)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = x_train.shape[1]\n",
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate noise for generator\n",
    "def make_noise(batch_size):\n",
    "    return torch.Tensor(np.random.normal(0,1/3,(batch_size,n))).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_summary(d_l, g_l,acc_r,acc_g, m =''):\n",
    "    n = np.arange(len(d_l))\n",
    "    title = 'Loss and Accuracy plot'+'\\n'+ m\n",
    "    title = title.replace('.pickle','')\n",
    "    fig, axs = plt.subplots(2,figsize=(19.20,10.80))\n",
    "\n",
    "    axs[0].set_title(title,fontsize=20.0,fontweight=\"bold\")\n",
    "    axs[0].plot(n, g_l,label='Generator loss',linewidth=4)\n",
    "    axs[0].plot(n, d_l,label='Discriminator loss',linewidth=4)\n",
    "    axs[0].legend(loc=0, prop={'size': 20})\n",
    "    axs[0].set_ylabel('Loss',fontsize=20.0,fontweight=\"bold\")\n",
    "    axs[0].tick_params(labelsize=20)\n",
    "    axs[0].tick_params(axis='x',which='both',bottom=False,top=False,labelbottom=False,labelsize=20)\n",
    "\n",
    "    # axs[1].plot(n, acc,'r',label='Discriminator accuracy',linewidth=4)\n",
    "    axs[1].plot(n, acc_g,label='Accuracy on Generated',linewidth=4)\n",
    "    axs[1].plot(n, acc_r,label='Accuracy on Real',linewidth=4)\n",
    "    axs[1].legend(loc=0,prop={'size': 20})\n",
    "    axs[1].set_ylabel('Accuracy',fontsize=20.0,fontweight=\"bold\")\n",
    "    axs[1].set_xlabel('Ephoc',fontsize=20.0,fontweight=\"bold\")\n",
    "    axs[1].tick_params(labelsize=20)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    #plt.show()\n",
    "    if not os.path.exists(\"imgs\"):\n",
    "        os.makedirs('imgs')\n",
    "    plt.savefig(f'imgs/{m[:-7]}.png',dpi = 300)\n",
    "    plt.close('all') #plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define the generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import pickle, itertools\n",
    "\n",
    "class CGAN(nn.Module):\n",
    "    def __init__(self, arguments, X, y):\n",
    "        \"\"\"Conditional Generative Adversarial Network class\"\"\"\n",
    "        super(CGAN, self).__init__()\n",
    "\n",
    "        # Unpack the arguments\n",
    "        [self.rand_noise_dim,self.n_layers, self.tot_epochs, self.batch_size, self.D_epochs, \n",
    "         self.G_epochs,  self.activation, \n",
    "         self.optimizer, self.learning_rate, self.min_num_neurons] = arguments\n",
    "\n",
    "        # Convert data to tensors\n",
    "        self.X_train = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y_train = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "        # Define data dimensions\n",
    "        self.label_dim = y.shape[1]\n",
    "        self.x_data_dim = X.shape[1]\n",
    "\n",
    "        # Initialize tracking variables\n",
    "        self.g_losses = []\n",
    "        self.d_losses, self.disc_loss_real, self.disc_loss_generated = [], [], []\n",
    "        self.acc_history = []\n",
    "        \n",
    "        # Define the models\n",
    "        self.generator = self.build_generator()\n",
    "        self.discriminator = self.build_discriminator()\n",
    "\n",
    "        # Optimizers for Generator and Discriminator\n",
    "        self.optim_G = optim.Adam(self.generator.parameters(), lr=self.learning_rate)\n",
    "        self.optim_D = optim.Adam(self.discriminator.parameters(), lr=self.learning_rate)\n",
    "\n",
    "        # Loss function\n",
    "        self.criterion = nn.BCELoss()\n",
    "\n",
    "        # For tracking purposes (naming the GAN based on arguments)\n",
    "        self.gan_name = '_'.join(str(e) for e in arguments).replace(\".\", \"\")\n",
    "        self.terminated = False\n",
    "\n",
    "    def build_generator(self):\n",
    "        \"\"\"Create the generator model\"\"\"\n",
    "        layers = []\n",
    "        # Input layer: noise + labels\n",
    "        layers.append(nn.Linear(self.rand_noise_dim + self.label_dim, self.min_num_neurons))\n",
    "        layers.append(self.get_activation(self.activation))\n",
    "        \n",
    "        # Hidden layers\n",
    "        for i in range(1, self.n_layers + 1):\n",
    "            layers.append(nn.Linear(self.min_num_neurons * i, self.min_num_neurons * (i + 1)))\n",
    "            layers.append(self.get_activation(self.activation))\n",
    "\n",
    "        # Output layer: generate data\n",
    "        layers.append(nn.Linear(self.min_num_neurons * (self.n_layers + 1), self.x_data_dim))\n",
    "        layers.append(nn.Sigmoid())  # Normalize output to (0, 1)\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def build_discriminator(self):\n",
    "        \"\"\"Create the discriminator model\"\"\"\n",
    "        layers = []\n",
    "        # Input layer: data + labels\n",
    "        layers.append(nn.Linear(self.x_data_dim + self.label_dim, self.min_num_neurons * self.n_layers))\n",
    "        layers.append(self.get_activation(self.activation))\n",
    "        \n",
    "        # Hidden layers (reverse order)\n",
    "        input_dim = self.min_num_neurons * self.n_layers\n",
    "        for n in reversed(range(1, self.n_layers + 1)):\n",
    "            layers.append(nn.Linear(input_dim, self.min_num_neurons * n))\n",
    "            layers.append(self.get_activation(self.activation))\n",
    "            input_dim = self.min_num_neurons * n\n",
    "        \n",
    "        # Output layer: classify as real or fake\n",
    "        layers.append(nn.Linear(self.min_num_neurons, 1))\n",
    "        layers.append(nn.Sigmoid())  # Output is probability\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def get_activation(self, activation_name):\n",
    "        \"\"\"Helper to get activation function\"\"\"\n",
    "        if activation_name == 'relu':\n",
    "            return nn.ReLU()\n",
    "        elif activation_name == 'tanh':\n",
    "            return nn.Tanh()\n",
    "        elif activation_name == 'leakyrelu':\n",
    "            return nn.LeakyReLU(0.2)\n",
    "        elif activation_name == 'softplus':\n",
    "            return nn.Softplus()  # 新增对 softplus 激活函数的支持\n",
    "        elif activation_name == 'linear':\n",
    "            return nn.Identity()  # linear 激活等同于不做任何变化，使用 nn.Identity()\n",
    "        elif activation_name == 'elu':\n",
    "            return nn.ELU()  # 新增对 ELU 激活函数的支持\n",
    "        else:\n",
    "            raise ValueError(f\"Activation {activation_name} not implemented\")\n",
    "\n",
    "    def get_batch(self):\n",
    "        \"\"\"Get a random batch of training data\"\"\"\n",
    "        batch_ix = np.random.choice(len(self.X_train), size=self.batch_size, replace=False)\n",
    "        return self.X_train[batch_ix], self.y_train[batch_ix]\n",
    "    \n",
    "    def save_model(self,model_dir = \"./models\"):\n",
    "        if not os.path.exists(model_dir):\n",
    "            os.makedirs(model_dir)\n",
    "\n",
    "         # 保存生成器\n",
    "        # self.generator.save(f\"{model_dir}/{self.gan_name}_generator_model.h5\")\n",
    "        torch.save(self.generator.state_dict(), f\"{model_dir}/{self.gan_name}_generator_model.pth\")\n",
    "\n",
    "        # 保存判别器\n",
    "        # self.discriminator.save(f\"{model_dir}/{self.gan_name}_discriminator_model.h5\")\n",
    "        torch.save(self.discriminator.state_dict(), f\"{model_dir}/{self.gan_name}_discriminator_model.pth\")\n",
    "\n",
    "        # 保存对抗模型（combined model）\n",
    "        # self.combined.save(f\"{model_dir}/{self.gan_name}_combined_model.h5\")\n",
    "        # torch.save(self.combined.state_dict(), f\"{model_dir}/{self.gan_name}_combined_model.pth\")\n",
    "\n",
    "    def dump_to_file(self, save_dir=\"./logs\"):\n",
    "        \"\"\"Save the training history to a file\"\"\"\n",
    "        H = defaultdict(dict)\n",
    "        H[\"acc_history\"] = self.acc_history\n",
    "        H[\"Generator_loss\"] = self.g_losses\n",
    "        H[\"disc_loss_real\"] = self.disc_loss_real\n",
    "        H[\"disc_loss_gen\"] = self.disc_loss_generated\n",
    "        H[\"discriminator_loss\"] = self.d_losses\n",
    "        H[\"rand_noise_dim\"], H[\"total_epochs\"] = self.rand_noise_dim, self.tot_epochs\n",
    "        H[\"batch_size\"], H[\"learning_rate\"] = self.batch_size, self.learning_rate\n",
    "        H[\"n_layers\"], H[\"activation\"] = self.n_layers, self.activation\n",
    "        H[\"optimizer\"], H[\"min_num_neurons\"] = self.optimizer, self.min_num_neurons\n",
    "\n",
    "        if not os.path.exists(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "\n",
    "\n",
    "        with open(f\"{save_dir}/CGAN_{self.gan_name}.pickle\", \"wb\") as output_file:\n",
    "            pickle.dump(H, output_file)\n",
    "\n",
    "        with open(f\"{save_dir}/CGAN_{self.gan_name}.pickle\", 'rb') as f:\n",
    "            x = pickle.load(f)\n",
    "\n",
    "        d_l = np.array(x['discriminator_loss']).ravel()\n",
    "        g_l = np.array(x['Generator_loss']).ravel()\n",
    "        acc_history = np.array(x['acc_history'])\n",
    "        acc = acc_history.sum(axis=1) * 0.5\n",
    "        acc_real = acc_history[:,1]\n",
    "        acc_gen = acc_history[:,0]\n",
    "\n",
    "        filename = f\"CGAN_{self.gan_name}.pickle\"\n",
    "        plot_summary(d_l, g_l,acc_real,acc_gen,filename)\n",
    "\n",
    "        if not os.path.exists(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "        \n",
    "        with open(f\"{save_dir}/{self.gan_name}{'.pickle'}\", \"wb\") as output_file:\n",
    "            pickle.dump(H,output_file)\n",
    "        \n",
    "        self.save_model()\n",
    "        \n",
    "            \n",
    "    # 改进 labels 的生成\n",
    "    def get_balanced_labels(self):\n",
    "        \"\"\"根据类别分布生成更多的少数类标签(3和4)\"\"\"\n",
    "        labels = []\n",
    "        for i in range(self.batch_size):\n",
    "            # 假设 3 和 4 是少数类，提高它们生成的概率\n",
    "            if np.random.rand() < 0.7:  # 设置少数类生成概率为 70%\n",
    "                labels.append(np.random.choice([3, 4]))  # 在少数类中随机选择一个\n",
    "            else:\n",
    "                labels.append(np.random.choice([0, 1, 2]))  # 在多数类中选择一个\n",
    "        return torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "    # 生成一些样本进行可视化\n",
    "    def visualize_samples(self, num_samples=100):\n",
    "        noise = torch.randn(num_samples, self.rand_noise_dim)\n",
    "        labels = torch.randint(0, 5, (num_samples, self.label_dim), dtype=torch.float32)\n",
    "        generated_samples = self.generator(torch.cat([noise, labels], 1))\n",
    "\n",
    "        # 可视化生成的样本 (可根据特征进行降维后展示)\n",
    "        plt.scatter(generated_samples[:, 0].detach().numpy(), generated_samples[:, 1].detach().numpy())\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"Train the CGAN model\"\"\"\n",
    "        real_labels = torch.ones(self.batch_size, 1)\n",
    "        fake_labels = torch.zeros(self.batch_size, 1)\n",
    "\n",
    "        for epoch in range(self.tot_epochs):\n",
    "            # Train Discriminator\n",
    "            for i in range(self.D_epochs):\n",
    "                x, labels = self.get_batch()\n",
    "                noise = torch.randn(self.batch_size, self.rand_noise_dim)\n",
    "                generated_x = self.generator(torch.cat([noise, labels], 1))\n",
    "\n",
    "                # Train on real and fake data\n",
    "                self.optim_D.zero_grad()\n",
    "                real_loss = self.criterion(self.discriminator(torch.cat([x, labels], 1)), real_labels)\n",
    "                fake_loss = self.criterion(self.discriminator(torch.cat([generated_x.detach(), labels], 1)), fake_labels)\n",
    "                d_loss = 0.5 * (real_loss + fake_loss)\n",
    "                d_loss.backward()\n",
    "                self.optim_D.step()\n",
    "\n",
    "            # Train Generator\n",
    "            for j in range(self.G_epochs):\n",
    "                self.optim_G.zero_grad()\n",
    "                noise = torch.randn(self.batch_size, self.rand_noise_dim)\n",
    "                labels = torch.randint(0, 5, (self.batch_size, self.label_dim), dtype=torch.float32)\n",
    "                generated_x = self.generator(torch.cat([noise, labels], 1))\n",
    "                g_loss = self.criterion(self.discriminator(torch.cat([generated_x, labels], 1)), real_labels)\n",
    "                g_loss.backward()\n",
    "                self.optim_G.step()\n",
    "\n",
    "            # Record losses for analysis\n",
    "            self.g_losses.append(g_loss.item())\n",
    "            self.d_losses.append(d_loss.item())\n",
    "            self.acc_history.append([real_loss.item(), fake_loss.item()])\n",
    "\n",
    "            # print(f\"Epoch {epoch} [D loss: {d_loss.item():.4f}, G loss: {g_loss.item():.4f}]\")\n",
    "\n",
    "            # Terminate if NaN is detected in loss\n",
    "            if torch.isnan(d_loss) or torch.isnan(g_loss):\n",
    "                self.terminated = True\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# set parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rand_dim = np.arange(10,110,10)\n",
    "# base_n_count = np.arange(3,41,3)\n",
    "# ephocs = np.arange(100,5000,100)\n",
    "# batch_sizes = [64,128,250,300,350]\n",
    "# learning_rates = np.logspace(-1,-4,num=20)\n",
    "# num_layers = np.arange(3,20)\n",
    "\n",
    "# optimizers = [\"sgd\", \"RMSprop\", \"adam\", \"Adagrad\", \"Adamax\",\"Nadam\"]\n",
    "# activation_func = [\"tanh\",\"relu\",\"softplus\",\"linear\",\"elu\"]\n",
    "\n",
    "\n",
    "base_n_count = [27] # 调整基本神经元数量的范围，从3-41变为5-51，步长为5\n",
    "ephocs = [2000]  # 调整周期范围，从100-5000变为500-5500，步长为500\n",
    "batch_sizes = [ 128]  # 简化批量大小选择为128, 256, 512\n",
    "learning_rates =  [0.0005]  # 调整学习率范围，从0.01到0.00001，共10个值\n",
    "num_layers = [4]  # 调整隐藏层数量，范围从2到10\n",
    "\n",
    "optimizers = [\"sgd\"]  # 精简优化器选项，去除一些效果可能较差的\n",
    "activation_func = [\"relu\"]  # 精简激活函数选项\n",
    "\n",
    "d_epochs = [1]\n",
    "g_epochs = [1]\n",
    "\n",
    "\n",
    "#create a logs directory\n",
    "if not os.path.exists('logs'):\n",
    "    os.makedirs('logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 生成所有可能的参数组合\n",
    "# tot = list(itertools.product([32],ephocs,batch_sizes,[1],[1],\\\n",
    "#                              learning_rates,num_layers,activation_func,optimizers,base_n_count))\n",
    "# for i in tot:\n",
    "#     args = list(i)\n",
    "#     cgan = CGAN(args,x,y)\n",
    "#     cgan.train()\n",
    "#     if not cgan.terminated :\n",
    "#         cgan.dump_to_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 1/1 [00:32<00:00, 32.02s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 生成所有可能的参数组合\n",
    "tot = list(itertools.product([32],num_layers,ephocs,batch_sizes,[1],[1],\\\n",
    "                            activation_func,optimizers,learning_rates,base_n_count))\n",
    "# for i in tot:\n",
    "#     args = list(i)\n",
    "#     cgan = CGAN(args,x,y)\n",
    "#     cgan.train()\n",
    "#     if not cgan.terminated :\n",
    "#         cgan.dump_to_file()\n",
    "\n",
    "\n",
    "# 使用 tqdm 显示进度条\n",
    "for idx, i in tqdm(enumerate(tot), total=len(tot), desc=\"Processing\"):\n",
    "    args = list(i)\n",
    "    cgan = CGAN(args, x, y)  # 假设你已经定义了 CGAN 类\n",
    "    cgan.train()  # 假设 train 方法存在\n",
    "    if not cgan.terminated:\n",
    "        cgan.dump_to_file()  # 假设 dump_to_file 方法存在\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of the data generated by CGAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preprocessing' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train,test, label_mapping \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocessing\u001b[49m\u001b[38;5;241m.\u001b[39mget_data(encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLabel\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m data_cols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(train\u001b[38;5;241m.\u001b[39mcolumns[ train\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m ])\n\u001b[0;32m      3\u001b[0m train \u001b[38;5;241m=\u001b[39m preprocessing\u001b[38;5;241m.\u001b[39mnormalize_data(train,data_cols)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'preprocessing' is not defined"
     ]
    }
   ],
   "source": [
    "train,test, label_mapping = preprocessing.get_data(encoding=\"Label\")\n",
    "data_cols = list(train.columns[ train.columns != 'label' ])\n",
    "train = preprocessing.normalize_data(train,data_cols)\n",
    "test = preprocessing.normalize_data(test,data_cols)\n",
    "x_train , x_test = preprocessing.preprocess(train,test,data_cols,\"Robust\",True)\n",
    "# x_train = preprocessing.remove_outliers(train)\n",
    "\n",
    "# train, test = None, None\n",
    "y_train = x_train.label.values\n",
    "y_test = x_test.label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cols = list(x_train.columns[ x_train.columns != 'label' ])\n",
    "to_drop = preprocessing.get_contant_featues(x_train,data_cols,threshold=0.995)\n",
    "x_train.drop(to_drop, axis=1,inplace=True)\n",
    "x_test.drop(to_drop, axis=1,inplace=True)\n",
    "data_cols = list(x_train.columns[ x_train.columns != 'label' ])\n",
    "\n",
    "print(\"Total data features : {}\".format(len(data_cols)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read keras model\n",
    "from keras.models import load_model\n",
    "model_path = './models/generator_model.h5'\n",
    "model = load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_df = x_train.query('label == 1')\n",
    "print(\"# of samples : {}\".format(normal_df.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1 = normal_df[data_cols].quantile(0.25)\n",
    "Q3 = normal_df[data_cols].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "print(IQR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "swarm_plot = sns.pairplot(train.query('label != 1'),hue='label')\n",
    "# fig = swarm_plot.get_figure()\n",
    "plt.savefig(\"all.png\",dpi = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dos_df = x_train.query('label == 0')\n",
    "print(\"Number of samples : {}\".format(dos_df.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1 = dos_df[data_cols].quantile(0.25)\n",
    "Q3 = dos_df[data_cols].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "# print(IQR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probe_df = x_train.query('label == 2')\n",
    "print(\"Number of probe attack samples : {}\".format(probe_df.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1 = probe_df[data_cols].quantile(0.25)\n",
    "Q3 = probe_df[data_cols].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "# print(IQR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the difference of fake and real attacks\n",
    "\n",
    "figure(num=None, figsize=(10, 8), dpi=200, facecolor='w', edgecolor='k')\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(probe_df.drop(\"label\",axis=1))\n",
    "decomposed_probe_df = pca.transform(probe_df.drop(\"label\",axis=1))\n",
    "\n",
    "noise = np.random.normal(0, 1, (len(probe_df), 32))\n",
    "generated_probe = model.predict([noise, probe_df.label.values.reshape(-1,1)])[:,:-1]\n",
    "\n",
    "gen_decomposed = PCA(n_components=2).fit_transform(generated_probe)\n",
    "plt.scatter(x=decomposed_probe_df[:,0],y=decomposed_probe_df[:,1],label=\"Real\")\n",
    "plt.scatter(x=gen_decomposed[:,0],y=gen_decomposed[:,1],label=\"Generated\")\n",
    "plt.title(\"Probe Real and Generated samples\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
