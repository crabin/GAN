{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle, itertools\n",
    "from collections import defaultdict\n",
    "\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CGAN(nn.Module):\n",
    "    def __init__(self, arguments, X, y):\n",
    "        \"\"\"Conditional Generative Adversarial Network class\"\"\"\n",
    "        super(CGAN, self).__init__()\n",
    "\n",
    "        [self.rand_noise_dim, self.tot_epochs, self.batch_size, self.D_epochs, \\\n",
    "         self.G_epochs, self.learning_rate, self.n_layers, self.activation, self.optimizer, self.min_num_neurons] = arguments\n",
    "\n",
    "        self.X_train = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y_train = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "        self.label_dim = y.shape[1]\n",
    "        self.x_data_dim = X.shape[1]\n",
    "\n",
    "        self.g_losses = []\n",
    "        self.d_losses, self.disc_loss_real, self.disc_loss_generated = [], [], []\n",
    "        self.acc_history = []\n",
    "        \n",
    "        # Define the models\n",
    "        self.generator = self.build_generator()\n",
    "        self.discriminator = self.build_discriminator(X)\n",
    "\n",
    "        self.optim_G = optim.Adam(self.generator.parameters(), lr=self.learning_rate)\n",
    "        self.optim_D = optim.Adam(self.discriminator.parameters(), lr=self.learning_rate)\n",
    "\n",
    "        self.criterion = nn.BCELoss()\n",
    "        self.gan_name = '_'.join(str(e) for e in arguments).replace(\".\", \"\")\n",
    "        \n",
    "        self.terminated = False\n",
    "\n",
    "    def build_generator(self):\n",
    "        \"\"\"Create the generator model\"\"\"\n",
    "        layers = []\n",
    "        layers.append(nn.Linear(self.rand_noise_dim + self.label_dim, self.min_num_neurons))\n",
    "        layers.append(self.get_activation(self.activation))\n",
    "        \n",
    "        for i in range(1, self.n_layers + 1):\n",
    "            layers.append(nn.Linear(self.min_num_neurons * i, self.min_num_neurons * (i + 1)))\n",
    "            layers.append(self.get_activation(self.activation))\n",
    "\n",
    "        layers.append(nn.Linear(self.min_num_neurons * (self.n_layers + 1), self.x_data_dim))\n",
    "        layers.append(nn.Sigmoid())\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def build_discriminator(self,x):\n",
    "        \"\"\"Create the discriminator model\"\"\"\n",
    "        layers = []\n",
    "        layers.append(nn.Linear(self.x_data_dim + self.label_dim, self.min_num_neurons * self.n_layers))\n",
    "        layers.append(self.get_activation(self.activation))\n",
    "        \n",
    "        # for n in reversed(range(1, self.n_layers + 1)):\n",
    "        #     layers.append(nn.Linear(self.min_num_neurons * n, self.min_num_neurons * (n - 1)))\n",
    "        #     layers.append(self.get_activation(self.activation))\n",
    "        \n",
    "        # layers.append(nn.Linear(self.min_num_neurons, 1))\n",
    "        # layers.append(nn.Sigmoid())\n",
    "\n",
    "        # return nn.Sequential(*layers)\n",
    "        input_dim = self.x_data_dim + self.label_dim  # 输入特征加上标签的维度\n",
    "        for n in reversed(range(1, self.n_layers + 1)):\n",
    "            x = nn.Linear(input_dim, self.min_num_neurons * n)(x)  # 确保输入维度正确\n",
    "            x = nn.LeakyReLU(0.2)(x)\n",
    "        \n",
    "        x = nn.Linear(self.min_num_neurons, 1)(x)\n",
    "        x = nn.Sigmoid()(x)\n",
    "        return x\n",
    "\n",
    "    def get_activation(self, activation_name):\n",
    "        \"\"\"Helper to get activation function\"\"\"\n",
    "        if activation_name == 'relu':\n",
    "            return nn.ReLU()\n",
    "        elif activation_name == 'tanh':\n",
    "            return nn.Tanh()\n",
    "        elif activation_name == 'leakyrelu':\n",
    "            return nn.LeakyReLU(0.2)\n",
    "        else:\n",
    "            raise ValueError(f\"Activation {activation_name} not implemented\")\n",
    "\n",
    "    def get_batch(self):\n",
    "        \"\"\"Get a random batch of training data\"\"\"\n",
    "        batch_ix = np.random.choice(len(self.X_train), size=self.batch_size, replace=False)\n",
    "        return self.X_train[batch_ix], self.y_train[batch_ix]\n",
    "\n",
    "    def dump_to_file(self, save_dir=\"./logs\"):\n",
    "        \"\"\"Save the training history to a file\"\"\"\n",
    "        H = defaultdict(dict)\n",
    "        H[\"acc_history\"] = self.acc_history\n",
    "        H[\"Generator_loss\"] = self.g_losses\n",
    "        H[\"disc_loss_real\"] = self.disc_loss_real\n",
    "        H[\"disc_loss_gen\"] = self.disc_loss_generated\n",
    "        H[\"discriminator_loss\"] = self.d_losses\n",
    "        H[\"rand_noise_dim\"], H[\"total_epochs\"] = self.rand_noise_dim, self.tot_epochs\n",
    "        H[\"batch_size\"], H[\"learning_rate\"] = self.batch_size, self.learning_rate\n",
    "        H[\"n_layers\"], H[\"activation\"] = self.n_layers, self.activation\n",
    "        H[\"optimizer\"], H[\"min_num_neurons\"] = self.optimizer, self.min_num_neurons\n",
    "\n",
    "        if not os.path.exists(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "\n",
    "        with open(f\"{save_dir}/{self.gan_name}.pickle\", \"wb\") as output_file:\n",
    "            pickle.dump(H, output_file)\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"Train the CGAN model\"\"\"\n",
    "        real_labels = Variable(torch.ones(self.batch_size, 1))\n",
    "        fake_labels = Variable(torch.zeros(self.batch_size, 1))\n",
    "\n",
    "        for epoch in range(self.tot_epochs):\n",
    "            # Train Discriminator\n",
    "            for i in range(self.D_epochs):\n",
    "                x, labels = self.get_batch()\n",
    "                noise = torch.randn(self.batch_size, self.rand_noise_dim)\n",
    "                generated_x = self.generator(torch.cat([noise, labels], 1))\n",
    "\n",
    "                # Train on real and fake data\n",
    "                self.optim_D.zero_grad()\n",
    "                real_loss = self.criterion(self.discriminator(torch.cat([x, labels], 1)), real_labels)\n",
    "                fake_loss = self.criterion(self.discriminator(torch.cat([generated_x.detach(), labels], 1)), fake_labels)\n",
    "                d_loss = 0.5 * (real_loss + fake_loss)\n",
    "                d_loss.backward()\n",
    "                self.optim_D.step()\n",
    "\n",
    "            # Train Generator\n",
    "            for j in range(self.G_epochs):\n",
    "                self.optim_G.zero_grad()\n",
    "                noise = torch.randn(self.batch_size, self.rand_noise_dim)\n",
    "                labels = torch.randint(0, 5, (self.batch_size, self.label_dim))\n",
    "                generated_x = self.generator(torch.cat([noise, labels], 1))\n",
    "                g_loss = self.criterion(self.discriminator(torch.cat([generated_x, labels], 1)), real_labels)\n",
    "                g_loss.backward()\n",
    "                self.optim_G.step()\n",
    "\n",
    "            self.g_losses.append(g_loss.item())\n",
    "            self.d_losses.append(d_loss.item())\n",
    "            self.acc_history.append([real_loss.item(), fake_loss.item()])\n",
    "\n",
    "            print(f\"Epoch {epoch} [D loss: {d_loss.item():.4f}, G loss: {g_loss.item():.4f}]\")\n",
    "\n",
    "            if torch.isnan(d_loss) or torch.isnan(g_loss):\n",
    "                self.terminated = True\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, StandardScaler ,MinMaxScaler,RobustScaler, PowerTransformer, normalize, OrdinalEncoder\n",
    "\n",
    "\n",
    "\n",
    "data_folder = \"./GANs_for_Network_Intrusion_Data/Data/NSL-KDD\"\n",
    "\n",
    "def get_data( data_folder = data_folder):\n",
    "    \"\"\"\n",
    "    Retrive Train and Test data\n",
    "    \"\"\"\n",
    "    print(os.getcwd())  # 打印当前工作目录\n",
    "    print(os.path.exists(data_folder))  # 应该输出 True\n",
    "\n",
    "    train = pd.read_csv(data_folder+\"/KDDTrain.csv\")\n",
    "    test = pd.read_csv(data_folder+\"/KDDTest.csv\")\n",
    "\n",
    "    # 将标签进行编码\n",
    "    le = LabelEncoder()\n",
    "    le.fit(train.label)\n",
    "    label_mapping = {l: i for i, l in enumerate(le.classes_)}\n",
    "\n",
    "    train['label'] = le.transform(train.label)\n",
    "    test['label'] = le.transform(test.label)\n",
    "\n",
    "    cols=[\"protocol_type\",\"service\",\"flag\"]\n",
    "    # 对数据进行Label编码\n",
    "    enc = OrdinalEncoder()\n",
    "    train[cols] = enc.fit_transform(train[cols])\n",
    "    test[cols] = enc.transform(test[cols])\n",
    "    \n",
    "    return train, test, label_mapping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess(x_train, x_test, data_cols, preprocessor = \"StandardScaler\",reject_features=False):\n",
    "    \"\"\"\n",
    "    Scale and transform data with an option to remove highly correlated features\n",
    "    \"\"\"\n",
    "    if reject_features :\n",
    "        # profile = pandas_profiling.ProfileReport(x_train)\n",
    "        # to_drop = profile.get_rejected_variables()\n",
    "        to_drop = ['dst_host_srv_serror_rate','num_root','rerror_rate',\n",
    "                    'serror_rate','srv_rerror_rate','srv_serror_rate']\n",
    "        x_train.drop(to_drop,axis=1,inplace=True)\n",
    "        x_test.drop(to_drop,axis=1,inplace=True)\n",
    "        data_cols = list(x_train.columns[ x_train.columns != 'label' ])\n",
    "\n",
    "    if preprocessor == \"MinMax\":\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        x_train[data_cols] = scaler.fit_transform(x_train[data_cols])\n",
    "        x_test[data_cols] = scaler.transform(x_test[data_cols])\n",
    "        return x_train, x_test\n",
    "\n",
    "    if preprocessor == \"Robust\":\n",
    "        scaler = RobustScaler(quantile_range=(0.1, 99.9))\n",
    "        x_train[data_cols] = scaler.fit_transform(x_train[data_cols])\n",
    "        x_test[data_cols] = scaler.transform(x_test[data_cols])\n",
    "        return x_train, x_test\n",
    "\n",
    "    if preprocessor == \"power_transform\":\n",
    "        pt = PowerTransformer(method=\"yeo-johnson\")\n",
    "        x_train[data_cols] = pt.fit_transform(x_train[data_cols])\n",
    "        x_test[data_cols] = pt.transform(x_test[data_cols])\n",
    "        return x_train, x_test\n",
    "\n",
    "    else :\n",
    "        scaler = StandardScaler()\n",
    "        x_train[data_cols] = scaler.fit_transform(x_train[data_cols])\n",
    "        x_test[data_cols] = scaler.transform(x_test[data_cols])\n",
    "        return x_train, x_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_contant_featues(X,data_cols,threshold=0.995):\n",
    "    \"\"\"\n",
    "    Finds columns with contant value\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    X : pandas DataFrame, shape = [n_samples, n_features]\n",
    "        Dataset to be analyzed\n",
    "    data_cols : List, array-like\n",
    "        feature names of the input data X\n",
    "    threshold : Float\n",
    "        threshold to determine if a feature has contant value\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    result : List , array-like\n",
    "        list of features having a contant value in Data X\n",
    "     \"\"\"\n",
    "    result = []\n",
    "    for col in data_cols:\n",
    "        val, counts = np.unique(X[col],return_counts=True)\n",
    "        v = counts[0]/counts.sum()\n",
    "        if v > threshold:\n",
    "            result.append(col)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\WorkSpace\\GAN\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "train,test, label_mapping = get_data()\n",
    "data_cols = list(train.columns[ train.columns != 'label' ])\n",
    "x_train , x_test = preprocess(train,test,data_cols,\"Robust\",True)\n",
    "\n",
    "y_train = x_train.label.values\n",
    "y_test = x_test.label.values\n",
    "\n",
    "data_cols = list(x_train.columns[ x_train.columns != 'label' ])\n",
    "\n",
    "to_drop = get_contant_featues(x_train,data_cols)\n",
    "x_train.drop(to_drop, axis=1,inplace=True)\n",
    "x_test.drop(to_drop, axis=1,inplace=True)\n",
    "\n",
    "data_cols = list(x_train.columns[ x_train.columns != 'label' ])\n",
    "\n",
    "att_ind = np.where(x_train.label != label_mapping[\"normal\"])[0]\n",
    "x = x_train[data_cols].values[att_ind]\n",
    "y = y_train[att_ind].reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rand_dim = np.arange(10,110,10)\n",
    "base_n_count = np.arange(3,41,3)\n",
    "ephocs = np.arange(100,5000,100)\n",
    "batch_sizes = [64,128,250,300,350]\n",
    "learning_rates = np.logspace(-1,-4,num=20)\n",
    "num_layers = np.arange(3,20)\n",
    "\n",
    "optimizers = [\"sgd\", \"RMSprop\", \"adam\", \"Adagrad\", \"Adamax\",\"Nadam\"]\n",
    "activation_func = [\"tanh\",\"relu\",\"softplus\",\"linear\",\"elu\"]\n",
    "\n",
    "#create a logs directory\n",
    "if not os.path.exists('logs'):\n",
    "    os.makedirs('logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sz124004\\AppData\\Local\\Temp\\ipykernel_4320\\4198243197.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.X_train = torch.tensor(X, dtype=torch.float32)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (58630x25 and 26x9)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[48], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tot:\n\u001b[0;32m      8\u001b[0m     args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(i)\n\u001b[1;32m----> 9\u001b[0m     cgan \u001b[38;5;241m=\u001b[39m \u001b[43mCGAN\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m     cgan\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m cgan\u001b[38;5;241m.\u001b[39mterminated :\n",
      "Cell \u001b[1;32mIn[42], line 21\u001b[0m, in \u001b[0;36mCGAN.__init__\u001b[1;34m(self, arguments, X, y)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Define the models\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_generator()\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdiscriminator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_discriminator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptim_G \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerator\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearning_rate)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptim_D \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdiscriminator\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearning_rate)\n",
      "Cell \u001b[1;32mIn[42], line 62\u001b[0m, in \u001b[0;36mCGAN.build_discriminator\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     60\u001b[0m input_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx_data_dim \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_dim  \u001b[38;5;66;03m# 输入特征加上标签的维度\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_layers \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)):\n\u001b[1;32m---> 62\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmin_num_neurons\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 确保输入维度正确\u001b[39;00m\n\u001b[0;32m     63\u001b[0m     x \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLeakyReLU(\u001b[38;5;241m0.2\u001b[39m)(x)\n\u001b[0;32m     65\u001b[0m x \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_num_neurons, \u001b[38;5;241m1\u001b[39m)(x)\n",
      "File \u001b[1;32md:\\environment\\anaconda3\\envs\\gan\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\environment\\anaconda3\\envs\\gan\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\environment\\anaconda3\\envs\\gan\\lib\\site-packages\\torch\\nn\\modules\\linear.py:117\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (58630x25 and 26x9)"
     ]
    }
   ],
   "source": [
    "# 确保 x 和 labels 都是 PyTorch Tensor\n",
    "x = torch.from_numpy(x).float()  # NumPy 转换为 Tensor\n",
    "\n",
    "\n",
    "tot = list(itertools.product([32],ephocs,batch_sizes,[1],[1],\\\n",
    "                             learning_rates,num_layers,activation_func,optimizers,base_n_count))\n",
    "for i in tot:\n",
    "    args = list(i)\n",
    "    cgan = CGAN(args,x,y)\n",
    "    cgan.train()\n",
    "    if not cgan.terminated :\n",
    "        cgan.dump_to_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
