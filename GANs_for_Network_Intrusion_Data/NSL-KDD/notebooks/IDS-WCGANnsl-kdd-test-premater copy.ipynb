{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Hyperparameter Tuning for Conditional GAN (NLS-KDD)<center/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../utils')\n",
    "sys.path.append('../models')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import pickle, os, itertools\n",
    "from tqdm import tqdm  # tqdm 用于显示进度条\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import utils , preprocessing  \n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.layers import Dense, Input, Dropout,concatenate\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.13.0\n"
     ]
    }
   ],
   "source": [
    "# !pip install tensorflow==1.13.0rc2\n",
    "\n",
    "# 禁用 TensorFlow 的进度条等信息\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # 只显示错误信息\n",
    "\n",
    "# 禁用 TensorFlow 的标准日志\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 629583435780078730\n",
      "xla_global_id: -1\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compute gradient penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_penalty(D, real_samples, real_samples_label, fake_samples, fake_samples_label):\n",
    "    # 去掉不必要的维度，将标签转换为 [batch_size, 1]\n",
    "    real_samples_label = tf.squeeze(tf.expand_dims(real_samples_label, axis=1), axis=-1)\n",
    "    fake_samples_label = tf.squeeze(tf.expand_dims(fake_samples_label, axis=1), axis=-1)\n",
    "\n",
    "    # 合并特征和标签\n",
    "    real_samples = tf.concat([real_samples, real_samples_label], axis=1)\n",
    "    fake_samples = tf.concat([fake_samples, fake_samples_label], axis=1)\n",
    "\n",
    "    # 获取随机权重\n",
    "    alpha = tf.random.uniform([real_samples.shape[0], 1], minval=0, maxval=1, dtype=real_samples.dtype)\n",
    "    alpha = tf.broadcast_to(alpha, tf.shape(real_samples))\n",
    "\n",
    "    # 确保 real_samples 和 fake_samples 是 float32 类型\n",
    "    real_samples = tf.cast(real_samples, tf.float32)\n",
    "    fake_samples = tf.cast(fake_samples, tf.float32)\n",
    "    alpha = tf.cast(alpha, tf.float32)\n",
    "\n",
    "    # 对真实样本和生成样本进行插值\n",
    "    interpolates = alpha * real_samples + (1 - alpha) * fake_samples\n",
    "    interpolates = tf.Variable(interpolates, trainable=True)  # 转换为可求导的变量\n",
    "\n",
    "    # 使用 GradientTape 计算梯度\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(interpolates)\n",
    "        \n",
    "        # 分离特征和标签\n",
    "        features = interpolates[:, :-1]\n",
    "        label = tf.cast(interpolates[:, -1], tf.float32)\n",
    "        \n",
    "        # 拼接 features 和 label\n",
    "        combined_input = tf.concat([features, tf.expand_dims(label, axis=1)], axis=1)\n",
    "        d_interpolates = D(combined_input)\n",
    "\n",
    "    # 创建中间变量\n",
    "    fake = tf.ones_like(d_interpolates)\n",
    "\n",
    "    # 计算插值样本输出的梯度\n",
    "    gradients = tape.gradient(d_interpolates, interpolates)\n",
    "    # if gradients is None:\n",
    "    #     raise ValueError(\"梯度计算失败，gradients 为 None，检查 `tape.watch(interpolates)` 的设置。\")\n",
    "\n",
    "    # 计算梯度的范数\n",
    "    gradients = tf.reshape(gradients, [tf.shape(gradients)[0], -1])\n",
    "    gradient_penalty = tf.reduce_mean((tf.norm(gradients, ord=2, axis=1) - 1) ** 2)\n",
    "    \n",
    "    return gradient_penalty\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_summary(d_l, g_l,acc_r,acc_g, m =''):\n",
    "    # n = np.arange(len(d_l))\n",
    "    # 确保 d_l 和 g_l 的长度一致\n",
    "    min_length = min(len(d_l), len(g_l), len(acc_r), len(acc_g))\n",
    "    d_l = d_l[:min_length]\n",
    "    g_l = g_l[:min_length]\n",
    "    acc_r = acc_r[:min_length]\n",
    "    acc_g = acc_g[:min_length]\n",
    "    n = np.arange(min_length)  # 保证 n 与数据长度一致\n",
    "\n",
    "    title = 'Loss and Accuracy plot'+'\\n'+ m\n",
    "    title = title.replace('.pickle','')\n",
    "    fig, axs = plt.subplots(2,figsize=(19.20,10.80))\n",
    "\n",
    "    axs[0].set_title(title,fontsize=20.0,fontweight=\"bold\")\n",
    "    axs[0].plot(n, g_l,label='Generator loss',linewidth=4)\n",
    "    axs[0].plot(n, d_l,label='Discriminator loss',linewidth=4)\n",
    "    axs[0].legend(loc=0, prop={'size': 20})\n",
    "    axs[0].set_ylabel('Loss',fontsize=20.0,fontweight=\"bold\")\n",
    "    axs[0].tick_params(labelsize=20)\n",
    "    axs[0].tick_params(axis='x',which='both',bottom=False,top=False,labelbottom=False,labelsize=20)\n",
    "\n",
    "    # axs[1].plot(n, acc,'r',label='Discriminator accuracy',linewidth=4)\n",
    "    axs[1].plot(n, acc_g,label='Accuracy on Generated',linewidth=4)\n",
    "    axs[1].plot(n, acc_r,label='Accuracy on Real',linewidth=4)\n",
    "    axs[1].legend(loc=0,prop={'size': 20})\n",
    "    axs[1].set_ylabel('Accuracy',fontsize=20.0,fontweight=\"bold\")\n",
    "    axs[1].set_xlabel('Ephoc',fontsize=20.0,fontweight=\"bold\")\n",
    "    axs[1].tick_params(labelsize=20)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    #plt.show()\n",
    "    if not os.path.exists(\"imgs\"):\n",
    "        os.makedirs('imgs')\n",
    "    plt.savefig(f'imgs/{m[:-7]}.png',dpi = 300)\n",
    "    plt.close('all') #plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional Generative Adversarial Network Class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WCGAN():\n",
    "    \"\"\"Conditinal Generative Adversarial Network class\"\"\"\n",
    "    \n",
    "    def __init__(self,arguments,X,y):\n",
    "        [self.rand_noise_dim, self.tot_epochs, self.batch_size,self.D_epochs, \\\n",
    "         self.G_epochs,self.learning_rate, self.n_layers, self.activation,self.optimizer, self.min_num_neurones,self.lambda_gp] = arguments\n",
    "\n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "        \n",
    "        self.label_dim = y.shape[1]\n",
    "        self.x_data_dim = X.shape[1]\n",
    "\n",
    "        self.g_losses = []\n",
    "        self.d_losses, self.disc_loss_real, self.disc_loss_generated, self.gp_losses = [], [], [], []\n",
    "        self.acc_history = []\n",
    "        \n",
    "        self.__define_models()\n",
    "        self.gan_name = '_'.join(str(e) for e in arguments).replace(\".\",\"\")\n",
    "        \n",
    "        self.terminated = False\n",
    "\n",
    "    def build_generator(self,x,labels):\n",
    "        \"\"\"Create the generator model G(z,l) : z -> random noise , l -> label (condition)\"\"\"\n",
    "        \n",
    "        x = concatenate([x,labels])\n",
    "        for i in range(1,self.n_layers+1):\n",
    "            x = Dense(self.min_num_neurones*i, activation=self.activation)(x)\n",
    "            \n",
    "        x = Dense(self.x_data_dim)(x)\n",
    "        x = concatenate([x,labels])\n",
    "\n",
    "        return x\n",
    "\n",
    "    def build_discriminator(self,x):\n",
    "        \"\"\"Create the discrimnator model D(G(z,l)) : z -> random noise , l -> label (condition)\"\"\"\n",
    "        \n",
    "        for n in reversed(range(1,self.n_layers+1)):\n",
    "            x = Dense(self.min_num_neurones*n, activation=self.activation)(x)\n",
    "        \n",
    "        x = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def __define_models(self):\n",
    "        \"\"\"Define Generator, Discriminator & combined model\"\"\"\n",
    "        \n",
    "        # Create & Compile generator\n",
    "        generator_input = Input(shape=(self.rand_noise_dim,))\n",
    "        labels_tensor = Input(shape=(self.label_dim,))\n",
    "        generator_output = self.build_generator(generator_input, labels_tensor)\n",
    "\n",
    "        self.generator = Model(inputs=[generator_input, labels_tensor], outputs=[generator_output], name='generator')\n",
    "        self.generator.compile(loss='binary_crossentropy',optimizer=self.optimizer, metrics=['accuracy'])\n",
    "        K.set_value(self.generator.optimizer.lr,self.learning_rate)\n",
    "        \n",
    "\n",
    "        # Create & Compile generator\n",
    "        discriminator_model_input = Input(shape=(self.x_data_dim + self.label_dim,))\n",
    "        discriminator_output = self.build_discriminator(discriminator_model_input)\n",
    "\n",
    "        self.discriminator = Model(inputs=[discriminator_model_input],outputs=[discriminator_output],name='discriminator')\n",
    "        self.discriminator.compile(loss='binary_crossentropy',optimizer=self.optimizer, metrics=['accuracy'])\n",
    "        K.set_value(self.discriminator.optimizer.lr,self.learning_rate)\n",
    "\n",
    "        # Build \"frozen discriminator\"\n",
    "        frozen_discriminator = Model(inputs=[discriminator_model_input],outputs=[discriminator_output],name='frozen_discriminator')\n",
    "        frozen_discriminator.trainable = False\n",
    "\n",
    "        # Debug 1/3: discriminator weights\n",
    "        n_disc_trainable = len(self.discriminator.trainable_weights)\n",
    "\n",
    "        # Debug 2/3: generator weights\n",
    "        n_gen_trainable = len(self.generator.trainable_weights)\n",
    "\n",
    "        # Build & compile combined model from frozen weights discriminator\n",
    "        combined_output = frozen_discriminator(generator_output)\n",
    "        self.combined = Model(inputs = [generator_input, labels_tensor],outputs = [combined_output],name='adversarial_model')\n",
    "        self.combined.compile(loss='binary_crossentropy',optimizer=self.optimizer, metrics=['accuracy'])\n",
    "        K.set_value(self.combined.optimizer.lr,self.learning_rate)\n",
    "\n",
    "        # Debug 3/3: compare if trainable weights correct\n",
    "        assert(len(self.discriminator.trainable_weights) == n_disc_trainable)\n",
    "        assert(len(self.combined.trainable_weights) == n_gen_trainable)\n",
    "        \n",
    "    def __get_batch_idx(self):\n",
    "        \"\"\"random selects batch_size samples indexes from training data\"\"\"\n",
    "        \n",
    "        batch_ix = np.random.choice(len(self.X_train), size=self.batch_size, replace=False)\n",
    "\n",
    "        return batch_ix\n",
    "    \n",
    "    def save_model(self,model_dir = \"./models\"):\n",
    "        if not os.path.exists(model_dir):\n",
    "            os.makedirs(model_dir)\n",
    "\n",
    "         # 保存生成器\n",
    "        self.generator.save(f\"{model_dir}/{self.gan_name}_generator_model.h5\")\n",
    "\n",
    "        # 保存判别器\n",
    "        self.discriminator.save(f\"{model_dir}/{self.gan_name}_discriminator_model.h5\")\n",
    "\n",
    "        # 保存对抗模型（combined model）\n",
    "        self.combined.save(f\"{model_dir}/{self.gan_name}_combined_model.h5\")\n",
    "    \n",
    "    def dump_to_file(self,save_dir = \"./logs\"):\n",
    "        \"\"\"Dumps the training history and GAN config to pickle file \"\"\"\n",
    "        \n",
    "        H = defaultdict(dict)\n",
    "        H[\"acc_history\"] = self.acc_history\n",
    "        H[\"Generator_loss\"] = self.g_losses\n",
    "        H[\"disc_loss_real\"] = self.disc_loss_real\n",
    "        H[\"disc_loss_gen\"] = self.disc_loss_generated\n",
    "        H[\"discriminator_loss\"] = self.d_losses\n",
    "        H[\"rand_noise_dim\"] , H[\"total_epochs\"] = self.rand_noise_dim, self.tot_epochs\n",
    "        H[\"batch_size\"] , H[\"learning_rate\"]  = self.batch_size, self.learning_rate\n",
    "        H[\"n_layers\"] , H[\"activation\"]  = self.n_layers, self.activation\n",
    "        H[\"optimizer\"] , H[\"min_num_neurones\"] = self.optimizer, self.min_num_neurones\n",
    "\n",
    "        # save images\n",
    "        with open(f\"{save_dir}/WCGAN_{self.gan_name}.pickle\", \"wb\") as output_file:\n",
    "            pickle.dump(H, output_file)\n",
    "\n",
    "        with open(f\"{save_dir}/WCGAN_{self.gan_name}.pickle\", 'rb') as f:\n",
    "            x = pickle.load(f)\n",
    "\n",
    "        d_l = np.array(x['discriminator_loss']).ravel()\n",
    "        g_l = np.array(x['Generator_loss']).ravel()\n",
    "        acc_history = np.array(x['acc_history'])\n",
    "        acc = acc_history.sum(axis=1) * 0.5\n",
    "        acc_real = acc_history[:,1]\n",
    "        acc_gen = acc_history[:,0]\n",
    "\n",
    "        filename = f\"WCGAN_{self.gan_name}.pickle\"\n",
    "        plot_summary(d_l, g_l,acc_real,acc_gen,filename)\n",
    "        \n",
    "        if not os.path.exists(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "        \n",
    "        with open(f\"{save_dir}/{self.gan_name}{'.pickle'}\", \"wb\") as output_file:\n",
    "            pickle.dump(H,output_file)\n",
    "        \n",
    "        self.save_model()\n",
    "        \n",
    "    def train(self):\n",
    "        \"\"\"Trains the WCGAN model\"\"\"\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        real_labels = np.ones((self.batch_size, 1))\n",
    "        fake_labels = np.zeros((self.batch_size, 1))\n",
    "\n",
    "        for epoch in range(self.tot_epochs):\n",
    "            # Train Discriminator\n",
    "            for _ in range(self.D_epochs):\n",
    "\n",
    "                # 获取一批真实样本\n",
    "                idx = self.__get_batch_idx()\n",
    "                real_samples, real_labels = self.X_train[idx], self.y_train[idx]\n",
    "\n",
    "                # 生成噪声和标签输入生成器\n",
    "                noise = np.random.normal(0, 1, (self.batch_size, self.rand_noise_dim))\n",
    "                generated_labels = np.random.choice([0, 2, 3, 4], (self.batch_size, 1), replace=True)\n",
    "\n",
    "                # 使用生成器生成假样本\n",
    "                fake_samples = self.generator.predict([noise, generated_labels])\n",
    "\n",
    "                # 分别训练判别器，输入真实和生成的样本\n",
    "                d_loss_real = self.discriminator.train_on_batch(\n",
    "                    np.concatenate([real_samples, real_labels], axis=1), np.ones((self.batch_size, 1))\n",
    "                )\n",
    "                d_loss_fake = self.discriminator.train_on_batch(\n",
    "                    np.concatenate([fake_samples, generated_labels], axis=1), np.zeros((self.batch_size, 1))\n",
    "                )\n",
    "\n",
    "                # 计算梯度惩罚\n",
    "                gp = compute_gradient_penalty(self.discriminator, real_samples, real_labels, fake_samples, generated_labels)\n",
    "                self.gp_losses.append(gp)\n",
    "\n",
    "                # 最终判别器损失\n",
    "                d_loss = 0.5 * (d_loss_real[0] + d_loss_fake[0]) + self.lambda_gp * gp\n",
    "\n",
    "                # 记录损失\n",
    "                self.disc_loss_real.append(d_loss_real[0])\n",
    "                self.disc_loss_generated.append(d_loss_fake[0])\n",
    "                self.d_losses.append(d_loss)\n",
    "                self.acc_history.append([d_loss_fake[1], d_loss_real[1]])\n",
    "\n",
    "                # 检查是否有 NaN，终止训练\n",
    "                if np.isnan(d_loss_real).any() or np.isnan(d_loss_fake).any():\n",
    "                    self.terminated = True\n",
    "                    break\n",
    "\n",
    "            # Train Generator (with discriminator frozen)\n",
    "            for _ in range(self.G_epochs):\n",
    "                # 生成随机标签和噪声输入\n",
    "                sampled_labels = np.random.choice([0, 2, 3, 4], (self.batch_size, 1), replace=True)\n",
    "                g_loss = self.combined.train_on_batch([noise, sampled_labels], real_labels)\n",
    "                self.g_losses.append(g_loss[0])\n",
    "\n",
    "            # 打印或记录训练进度\n",
    "            print(f\"Epoch {epoch + 1}/{self.tot_epochs} - D Loss: {d_loss:.4f}, G Loss: {g_loss[0]:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read & Preprocess Input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\WorkSpace\\GAN\\GANs_for_Network_Intrusion_Data\\NSL-KDD\\notebooks\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "train,test, label_mapping = preprocessing.get_data(encoding=\"Label\")\n",
    "data_cols = list(train.columns[ train.columns != 'label' ])\n",
    "x_train , x_test = preprocessing.preprocess(train,test,data_cols,\"Robust\",True)\n",
    "\n",
    "y_train = x_train.label.values\n",
    "y_test = x_test.label.values\n",
    "\n",
    "data_cols = list(x_train.columns[ x_train.columns != 'label' ])\n",
    "\n",
    "to_drop = preprocessing.get_contant_featues(x_train,data_cols)\n",
    "x_train.drop(to_drop, axis=1,inplace=True)\n",
    "x_test.drop(to_drop, axis=1,inplace=True)\n",
    "\n",
    "data_cols = list(x_train.columns[ x_train.columns != 'label' ])\n",
    "\n",
    "att_ind = np.where(x_train.label != label_mapping[\"normal\"])[0]\n",
    "x = x_train[data_cols].values[att_ind]\n",
    "y = y_train[att_ind].reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set parameters to tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rand_dim = np.arange(10,110,10)\n",
    "# base_n_count = np.arange(3,41,3)\n",
    "# ephocs = np.arange(100,5000,100)\n",
    "# batch_sizes = [64,128,250,300,350]\n",
    "# learning_rates = np.logspace(-1,-4,num=20)\n",
    "# num_layers = np.arange(3,20)\n",
    "\n",
    "# optimizers = [\"sgd\", \"RMSprop\", \"adam\", \"Adagrad\", \"Adamax\",\"Nadam\"]\n",
    "# activation_func = [\"tanh\",\"relu\",\"softplus\",\"linear\",\"elu\"]\n",
    "\n",
    "#create a logs directory\n",
    "if not os.path.exists('logs'):\n",
    "    os.makedirs('logs')\n",
    "\n",
    "\n",
    "# rand_dim = np.arange(10,110,10)\n",
    "# base_n_count = np.arange(5,51,5)  # 调整基本神经元数量的范围，从3-41变为5-51，步长为5\n",
    "# ephocs = np.arange(500,5500,500)  # 调整周期范围，从100-5000变为500-5500，步长为500\n",
    "# batch_sizes = [128, 256, 512]  # 简化批量大小选择为128, 256, 512\n",
    "# learning_rates = np.logspace(-2,-5,num=10)  # 调整学习率范围，从0.01到0.00001，共10个值\n",
    "# num_layers = np.arange(2,10)  # 调整隐藏层数量，范围从2到10\n",
    "\n",
    "# optimizers = [\"adam\", \"RMSprop\", \"Nadam\"]  # 精简优化器选项，去除一些效果可能较差的\n",
    "# activation_func = [\"relu\", \"tanh\", \"elu\"]  # 精简激活函数选项\n",
    "\n",
    "# rand_dim = np.arange(10,110,10)\n",
    "base_n_count = [27]  # 调整基本神经元数量的范围，从3-41变为5-51，步长为5\n",
    "ephocs = [1000]  # 调整周期范围，从100-5000变为500-5500，步长为500\n",
    "batch_sizes = [128]  # 简化批量大小选择为128, 256, 512\n",
    "learning_rates = [0.005]  # 调整学习率范围，从0.01到0.00001，共10个值\n",
    "num_layers = [4]  # 调整隐藏层数量，范围从2到10\n",
    "lambda_gp = [10]  # 调整lambda_gp的值1\n",
    "optimizers = [\"sgd\"]  # 精简优化器选项，去除一些效果可能较差的\n",
    "activation_func = [\"tanh\"]  # 精简激活函数选项\n",
    "d_ephocs = [5]\n",
    "g_ephocs = [1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   0%|          | 0/1 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node 'discriminator/dense_25/BiasAdd' defined at (most recent call last):\n    File \"d:\\environment\\anaconda3\\envs\\gan\\lib\\runpy.py\", line 194, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"d:\\environment\\anaconda3\\envs\\gan\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"d:\\environment\\anaconda3\\envs\\gan\\lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n      app.launch_new_instance()\n    File \"d:\\environment\\anaconda3\\envs\\gan\\lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n      app.start()\n    File \"d:\\environment\\anaconda3\\envs\\gan\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n      self.io_loop.start()\n    File \"d:\\environment\\anaconda3\\envs\\gan\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n      self.asyncio_loop.run_forever()\n    File \"d:\\environment\\anaconda3\\envs\\gan\\lib\\asyncio\\base_events.py\", line 570, in run_forever\n      self._run_once()\n    File \"d:\\environment\\anaconda3\\envs\\gan\\lib\\asyncio\\base_events.py\", line 1859, in _run_once\n      handle._run()\n    File \"d:\\environment\\anaconda3\\envs\\gan\\lib\\asyncio\\events.py\", line 81, in _run\n      self._context.run(self._callback, *self._args)\n    File \"d:\\environment\\anaconda3\\envs\\gan\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n      await self.process_one()\n    File \"d:\\environment\\anaconda3\\envs\\gan\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n      await dispatch(*args)\n    File \"d:\\environment\\anaconda3\\envs\\gan\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n      await result\n    File \"d:\\environment\\anaconda3\\envs\\gan\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n      await super().execute_request(stream, ident, parent)\n    File \"d:\\environment\\anaconda3\\envs\\gan\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n      reply_content = await reply_content\n    File \"d:\\environment\\anaconda3\\envs\\gan\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n      res = shell.run_cell(\n    File \"d:\\environment\\anaconda3\\envs\\gan\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"d:\\environment\\anaconda3\\envs\\gan\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3009, in run_cell\n      result = self._run_cell(\n    File \"d:\\environment\\anaconda3\\envs\\gan\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3064, in _run_cell\n      result = runner(coro)\n    File \"d:\\environment\\anaconda3\\envs\\gan\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"d:\\environment\\anaconda3\\envs\\gan\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3269, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"d:\\environment\\anaconda3\\envs\\gan\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3448, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"d:\\environment\\anaconda3\\envs\\gan\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3508, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\sz124004\\AppData\\Local\\Temp\\ipykernel_8108\\3885429229.py\", line 8, in <module>\n      wcgan.train()  # 开始训练\n    File \"C:\\Users\\sz124004\\AppData\\Local\\Temp\\ipykernel_8108\\1049542877.py\", line 171, in train\n      d_loss_fake = self.discriminator.train_on_batch(\n    File \"d:\\environment\\anaconda3\\envs\\gan\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 2684, in train_on_batch\n      logs = self.train_function(iterator)\n    File \"d:\\environment\\anaconda3\\envs\\gan\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1338, in train_function\n      return step_function(self, iterator)\n    File \"d:\\environment\\anaconda3\\envs\\gan\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1322, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"d:\\environment\\anaconda3\\envs\\gan\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1303, in run_step\n      outputs = model.train_step(data)\n    File \"d:\\environment\\anaconda3\\envs\\gan\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1080, in train_step\n      y_pred = self(x, training=True)\n    File \"d:\\environment\\anaconda3\\envs\\gan\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"d:\\environment\\anaconda3\\envs\\gan\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 569, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"d:\\environment\\anaconda3\\envs\\gan\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"d:\\environment\\anaconda3\\envs\\gan\\lib\\site-packages\\keras\\src\\engine\\base_layer.py\", line 1150, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"d:\\environment\\anaconda3\\envs\\gan\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"d:\\environment\\anaconda3\\envs\\gan\\lib\\site-packages\\keras\\src\\engine\\functional.py\", line 512, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"d:\\environment\\anaconda3\\envs\\gan\\lib\\site-packages\\keras\\src\\engine\\functional.py\", line 669, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"d:\\environment\\anaconda3\\envs\\gan\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"d:\\environment\\anaconda3\\envs\\gan\\lib\\site-packages\\keras\\src\\engine\\base_layer.py\", line 1150, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"d:\\environment\\anaconda3\\envs\\gan\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"d:\\environment\\anaconda3\\envs\\gan\\lib\\site-packages\\keras\\src\\layers\\core\\dense.py\", line 252, in call\n      outputs = tf.nn.bias_add(outputs, self.bias)\nNode: 'discriminator/dense_25/BiasAdd'\nMatrix size-incompatible: In[0]: [128,27], In[1]: [26,108]\n\t [[{{node discriminator/dense_25/BiasAdd}}]] [Op:__inference_train_function_4345]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(i)\n\u001b[0;32m      7\u001b[0m wcgan \u001b[38;5;241m=\u001b[39m WCGAN(args, x, y)  \u001b[38;5;66;03m# 假设 CGAN 类已经定义好\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[43mwcgan\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 开始训练\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m wcgan\u001b[38;5;241m.\u001b[39mterminated:  \u001b[38;5;66;03m# 检查训练是否终止\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     wcgan\u001b[38;5;241m.\u001b[39mdump_to_file()  \u001b[38;5;66;03m# 保存训练结果\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[24], line 171\u001b[0m, in \u001b[0;36mWCGAN.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;66;03m# 分别训练判别器，输入真实和生成的样本\u001b[39;00m\n\u001b[0;32m    168\u001b[0m d_loss_real \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdiscriminator\u001b[38;5;241m.\u001b[39mtrain_on_batch(\n\u001b[0;32m    169\u001b[0m     np\u001b[38;5;241m.\u001b[39mconcatenate([real_samples, real_labels], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), np\u001b[38;5;241m.\u001b[39mones((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m    170\u001b[0m )\n\u001b[1;32m--> 171\u001b[0m d_loss_fake \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiscriminator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_on_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfake_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerated_labels\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;66;03m# 计算梯度惩罚\u001b[39;00m\n\u001b[0;32m    176\u001b[0m gp \u001b[38;5;241m=\u001b[39m compute_gradient_penalty(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdiscriminator, real_samples, real_labels, fake_samples, generated_labels)\n",
      "File \u001b[1;32md:\\environment\\anaconda3\\envs\\gan\\lib\\site-packages\\keras\\src\\engine\\training.py:2684\u001b[0m, in \u001b[0;36mModel.train_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight, reset_metrics, return_dict)\u001b[0m\n\u001b[0;32m   2680\u001b[0m     iterator \u001b[38;5;241m=\u001b[39m data_adapter\u001b[38;5;241m.\u001b[39msingle_batch_iterator(\n\u001b[0;32m   2681\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute_strategy, x, y, sample_weight, class_weight\n\u001b[0;32m   2682\u001b[0m     )\n\u001b[0;32m   2683\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_train_function()\n\u001b[1;32m-> 2684\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2686\u001b[0m logs \u001b[38;5;241m=\u001b[39m tf_utils\u001b[38;5;241m.\u001b[39msync_to_numpy_or_python_type(logs)\n\u001b[0;32m   2687\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_dict:\n",
      "File \u001b[1;32md:\\environment\\anaconda3\\envs\\gan\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32md:\\environment\\anaconda3\\envs\\gan\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node 'discriminator/dense_25/BiasAdd' defined at (most recent call last):\n    File \"d:\\environment\\anaconda3\\envs\\gan\\lib\\runpy.py\", line 194, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"d:\\environment\\anaconda3\\envs\\gan\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"d:\\environment\\anaconda3\\envs\\gan\\lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n      app.launch_new_instance()\n    File \"d:\\environment\\anaconda3\\envs\\gan\\lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n      app.start()\n    File \"d:\\environment\\anaconda3\\envs\\gan\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n      self.io_loop.start()\n    File \"d:\\environment\\anaconda3\\envs\\gan\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n      self.asyncio_loop.run_forever()\n    File \"d:\\environment\\anaconda3\\envs\\gan\\lib\\asyncio\\base_events.py\", line 570, in run_forever\n      self._run_once()\n    File \"d:\\environment\\anaconda3\\envs\\gan\\lib\\asyncio\\base_events.py\", line 1859, in _run_once\n      handle._run()\n    File \"d:\\environment\\anaconda3\\envs\\gan\\lib\\asyncio\\events.py\", line 81, in _run\n      self._context.run(self._callback, *self._args)\n    File \"d:\\environment\\anaconda3\\envs\\gan\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n      await self.process_one()\n    File \"d:\\environment\\anaconda3\\envs\\gan\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n      await dispatch(*args)\n    File \"d:\\environment\\anaconda3\\envs\\gan\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n      await result\n    File \"d:\\environment\\anaconda3\\envs\\gan\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n      await super().execute_request(stream, ident, parent)\n    File \"d:\\environment\\anaconda3\\envs\\gan\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n      reply_content = await reply_content\n    File \"d:\\environment\\anaconda3\\envs\\gan\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n      res = shell.run_cell(\n    File \"d:\\environment\\anaconda3\\envs\\gan\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"d:\\environment\\anaconda3\\envs\\gan\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3009, in run_cell\n      result = self._run_cell(\n    File \"d:\\environment\\anaconda3\\envs\\gan\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3064, in _run_cell\n      result = runner(coro)\n    File \"d:\\environment\\anaconda3\\envs\\gan\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"d:\\environment\\anaconda3\\envs\\gan\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3269, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"d:\\environment\\anaconda3\\envs\\gan\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3448, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"d:\\environment\\anaconda3\\envs\\gan\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3508, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\sz124004\\AppData\\Local\\Temp\\ipykernel_8108\\3885429229.py\", line 8, in <module>\n      wcgan.train()  # 开始训练\n    File \"C:\\Users\\sz124004\\AppData\\Local\\Temp\\ipykernel_8108\\1049542877.py\", line 171, in train\n      d_loss_fake = self.discriminator.train_on_batch(\n    File \"d:\\environment\\anaconda3\\envs\\gan\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 2684, in train_on_batch\n      logs = self.train_function(iterator)\n    File \"d:\\environment\\anaconda3\\envs\\gan\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1338, in train_function\n      return step_function(self, iterator)\n    File \"d:\\environment\\anaconda3\\envs\\gan\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1322, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"d:\\environment\\anaconda3\\envs\\gan\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1303, in run_step\n      outputs = model.train_step(data)\n    File \"d:\\environment\\anaconda3\\envs\\gan\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1080, in train_step\n      y_pred = self(x, training=True)\n    File \"d:\\environment\\anaconda3\\envs\\gan\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"d:\\environment\\anaconda3\\envs\\gan\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 569, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"d:\\environment\\anaconda3\\envs\\gan\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"d:\\environment\\anaconda3\\envs\\gan\\lib\\site-packages\\keras\\src\\engine\\base_layer.py\", line 1150, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"d:\\environment\\anaconda3\\envs\\gan\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"d:\\environment\\anaconda3\\envs\\gan\\lib\\site-packages\\keras\\src\\engine\\functional.py\", line 512, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"d:\\environment\\anaconda3\\envs\\gan\\lib\\site-packages\\keras\\src\\engine\\functional.py\", line 669, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"d:\\environment\\anaconda3\\envs\\gan\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"d:\\environment\\anaconda3\\envs\\gan\\lib\\site-packages\\keras\\src\\engine\\base_layer.py\", line 1150, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"d:\\environment\\anaconda3\\envs\\gan\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"d:\\environment\\anaconda3\\envs\\gan\\lib\\site-packages\\keras\\src\\layers\\core\\dense.py\", line 252, in call\n      outputs = tf.nn.bias_add(outputs, self.bias)\nNode: 'discriminator/dense_25/BiasAdd'\nMatrix size-incompatible: In[0]: [128,27], In[1]: [26,108]\n\t [[{{node discriminator/dense_25/BiasAdd}}]] [Op:__inference_train_function_4345]"
     ]
    }
   ],
   "source": [
    "tot = list(itertools.product([32], ephocs, batch_sizes, d_ephocs, g_ephocs,\n",
    "                             learning_rates, num_layers, activation_func, optimizers, base_n_count,lambda_gp))\n",
    "\n",
    "# 遍历所有组合并训练模型\n",
    "for idx, i in tqdm(enumerate(tot), total=len(tot), desc=\"Processing\"):\n",
    "    args = list(i)\n",
    "    wcgan = WCGAN(args, x, y)  # 假设 CGAN 类已经定义好\n",
    "    wcgan.train()  # 开始训练\n",
    "    if not wcgan.terminated:  # 检查训练是否终止\n",
    "        wcgan.dump_to_file()  # 保存训练结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ep_d = 1\n",
    "# ep_g = 1\n",
    "\n",
    "# for ep in ephocs:\n",
    "#     for lr in learning_rates:\n",
    "#         for op in optimizers:\n",
    "#             for ac in activation_func:\n",
    "#                 for n_layers in num_layers:\n",
    "#                     for base_n in base_n_count:\n",
    "#                         for BS in batch_sizes:\n",
    "#                             args = [32, ep, BS, ep_d ,ep_g, lr, n_layers, ac ,op, base_n]\n",
    "#                             cgan = CGAN(args,x,y)\n",
    "#                             cgan.train()\n",
    "#                             cgan.dump_to_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# args = [32, 500, 130,1 ,1, 0.001, 4, \"tanh\" ,\"sgd\", 50]\n",
    "# cgan = CGAN(args,x,y.reshape(-1,1))\n",
    "# cgan.train()\n",
    "# cgan.dump_to_file()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
